{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import PIL.Image as Image\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import importlib\n",
    "from plot_utils.MNIST_plot_utils import scale_to_unit_interval, save_ten_images, plot_ten_images, tile_raster_images\n",
    "from plot_utils.ts_plot_utils import plot_ts, plot_ts_recon, save_ts, save_ts_recon\n",
    "from plot_utils.heatmap import heatmap, annotate_heatmap\n",
    "\n",
    "seed=30\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequences(ts, window):\n",
    "    shape = (ts.size - window + 1, window)\n",
    "    strides = ts.strides * 2\n",
    "    return np.lib.stride_tricks.as_strided(ts, shape=shape, strides=strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox_l1(lam, x):\n",
    "    return (x > lam) * (x - lam) + (x < -lam) * (x + lam)\n",
    "\n",
    "def prox_l21(lam, x):\n",
    "    e = np.linalg.norm(x, axis=0, keepdims=False)\n",
    "    for i in range(len(e)):\n",
    "        if e[i] > lam:\n",
    "            x[:,i] = x[:,i] - lam*e[i]\n",
    "        else:\n",
    "            x[:,i] = np.zeros(len(x[:,i]))\n",
    "    return x\n",
    "    \n",
    "\n",
    "def get_Dense_encoder(input_size, dense_units):\n",
    "    encoder = tf.keras.Sequential()\n",
    "    encoder.add(layers.Input(shape=(input_size)))\n",
    "    for i in range(len(dense_units)):\n",
    "        encoder.add(layers.Dense(units=dense_units[i], activation='relu'))\n",
    "    return encoder\n",
    "\n",
    "def get_Dense_decoder(input_size, dense_units):\n",
    "    decoder = tf.keras.Sequential()\n",
    "    decoder.add(layers.Input(shape=(dense_units[-1])))\n",
    "    for i in reversed(range(len(dense_units)-1)):\n",
    "        decoder.add(layers.Dense(units=dense_units[i], activation='relu'))\n",
    "    decoder.add(layers.Dense(units=input_size, activation='sigmoid'))\n",
    "    return decoder\n",
    "\n",
    "def get_LSTM_encoder(timesteps, features, LSTM_units, LSTM_dropout):\n",
    "    encoder = tf.keras.Sequential()\n",
    "    if len(LSTM_units) > 0:\n",
    "        encoder.add(layers.LSTM(units=LSTM_units[0], dropout=LSTM_dropout, return_sequences=True, input_shape=(timesteps, features)))\n",
    "        for i in range(len(LSTM_units)-2):\n",
    "            encoder.add(layers.LSTM(units=LSTM_units[i+1], dropout=LSTM_dropout, return_sequences=True))\n",
    "        encoder.add(layers.LSTM(units=LSTM_units[-1], dropout=LSTM_dropout, return_sequences=False))\n",
    "    else:\n",
    "        encoder.add(layers.LSTM(units=LSTM_units[0], dropout=LSTM_dropout, return_sequences=False, input_shape=(timesteps, features)))            \n",
    "    return encoder\n",
    "\n",
    "def get_LSTM_decoder(timesteps, features, LSTM_units, LSTM_dropout):\n",
    "    decoder = tf.keras.Sequential()\n",
    "    decoder.add(layers.RepeatVector(timesteps))\n",
    "    for i in reversed(range(len(LSTM_units))):\n",
    "        decoder.add(layers.LSTM(units=LSTM_units[i], dropout=LSTM_dropout, return_sequences=True))\n",
    "    decoder.add(layers.TimeDistributed(layer=layers.Dense(units=features, activation='sigmoid')))\n",
    "    return decoder\n",
    "\n",
    "\n",
    "def get_GRU_encoder(timesteps, features, GRU_units, GRU_dropout):\n",
    "    encoder = tf.keras.Sequential()\n",
    "    if len(GRU_units) > 0:\n",
    "        encoder.add(layers.GRU(units=GRU_units[0], dropout=GRU_dropout, return_sequences=True, input_shape=(timesteps, features)))\n",
    "        for i in range(len(GRU_units)-2):\n",
    "            encoder.add(layers.GRU(units=GRU_units[i+1], dropout=GRU_dropout, return_sequences=True))\n",
    "        encoder.add(layers.GRU(units=GRU_units[-1], dropout=GRU_dropout, return_sequences=False))\n",
    "    else:\n",
    "        encoder.add(layers.GRU(units=GRU_units[0], dropout=GRU_dropout, return_sequences=False, input_shape=(timesteps, features)))            \n",
    "    return encoder\n",
    "\n",
    "def get_GRU_decoder(timesteps, features, GRU_units, GRU_dropout):\n",
    "    decoder = tf.keras.Sequential()\n",
    "    decoder.add(layers.RepeatVector(timesteps))\n",
    "    for i in reversed(range(len(GRU_units))):\n",
    "        decoder.add(layers.GRU(units=GRU_units[i], dropout=GRU_dropout, return_sequences=True))\n",
    "    decoder.add(layers.TimeDistributed(layer=layers.Dense(units=features, activation='sigmoid')))\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Deep Dense Autoencoder Model\n",
    "class DAE_Dense(Model):\n",
    "    def __init__(self, input_size, dense_units):\n",
    "        super(DAE_Dense, self).__init__()\n",
    "        self.encoder = get_Dense_encoder(input_size, dense_units)\n",
    "        self.decoder = get_Dense_decoder(input_size, dense_units)\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        encoded = self.encoder(x, training=training)\n",
    "        decoded = self.decoder(encoded, training=training)\n",
    "        return decoded\n",
    "\n",
    "    def encode(self, x, training=False):\n",
    "        encoded = self.encoder(x, training=training)\n",
    "        return encoded\n",
    "    \n",
    "    def decode(self, x, training=False):\n",
    "        decoded = self.decoder(x, training=training)\n",
    "        return decoded\n",
    "    \n",
    "    def train_step(self, x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Reconstruct input\n",
    "            x_encoded = self.encode(x, training=True)\n",
    "            x_recon = self.decode(x_encoded, training=True)\n",
    "            # Calculate loss\n",
    "            loss = self.compiled_loss(x, x_recon)\n",
    "            \n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(x, x_recon)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "### Deep LSTM Autoencoder Model\n",
    "class DAE_LSTM(Model):\n",
    "    def __init__(self, timesteps, features, LSTM_units, LSTM_dropout):\n",
    "        super(DAE_LSTM, self).__init__()\n",
    "        self.encoder = get_LSTM_encoder(timesteps, features, LSTM_units, LSTM_dropout)\n",
    "        self.decoder = get_LSTM_decoder(timesteps, features, LSTM_units, LSTM_dropout)\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        encoded = self.encoder(x, training=training)\n",
    "        decoded = self.decoder(encoded, training=training)\n",
    "        return decoded\n",
    "\n",
    "    def encode(self, x, training=False):\n",
    "        encoded = self.encoder(x, training=training)\n",
    "        return encoded\n",
    "    \n",
    "    def decode(self, x, training=False):\n",
    "        decoded = self.decoder(x, training=training)\n",
    "        return decoded\n",
    "    \n",
    "    def train_step(self, x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Reconstruct input\n",
    "            x_encoded = self.encode(x, training=True)\n",
    "            x_recon = self.decode(x_encoded, training=True)\n",
    "            # Calculate loss\n",
    "            loss = self.compiled_loss(x, x_recon)\n",
    "            \n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(x, x_recon)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    \n",
    "### Deep GRU Autoencoder Model\n",
    "class DAE_GRU(Model):\n",
    "    def __init__(self, timesteps, features, GRU_units, GRU_dropout):\n",
    "        super(DAE_GRU, self).__init__()\n",
    "        self.encoder = get_GRU_encoder(timesteps, features, GRU_units, GRU_dropout)\n",
    "        self.decoder = get_GRU_decoder(timesteps, features, GRU_units, GRU_dropout)\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        encoded = self.encoder(x, training=training)\n",
    "        decoded = self.decoder(encoded, training=training)\n",
    "        return decoded\n",
    "\n",
    "    def encode(self, x, training=False):\n",
    "        encoded = self.encoder(x, training=training)\n",
    "        return encoded\n",
    "    \n",
    "    def decode(self, x, training=False):\n",
    "        decoded = self.decoder(x, training=training)\n",
    "        return decoded\n",
    "    \n",
    "    def train_step(self, x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Reconstruct input\n",
    "            x_encoded = self.encode(x, training=True)\n",
    "            x_recon = self.decode(x_encoded, training=True)\n",
    "            # Calculate loss\n",
    "            loss = self.compiled_loss(x, x_recon)\n",
    "            \n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(x, x_recon)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Robust Autoencoder Model\n",
    "class RobustAutoencoder:\n",
    "    def __init__(self, AE_type: str, prox_type: str, input_size=784, dense_units=[200, 10], lr=3e-4, timesteps=24, features=1, LSTM_units=[64, 32], LSTM_dropout=0.0, GRU_units=[64,32], GRU_dropout=0.0):\n",
    "        super(RobustAutoencoder, self).__init__()\n",
    "        assert AE_type=='Dense' or AE_type=='LSTM' or AE_type=='GRU', 'AE_type has to be either Dense or LSTM or GRU'\n",
    "        self.AE_type = AE_type\n",
    "        \n",
    "        assert prox_type=='l1' or prox_type=='l21', 'prox_type has to be either l1 or l21'\n",
    "        self.prox_type = prox_type\n",
    "        \n",
    "        if self.AE_type=='Dense':\n",
    "            self.AE = DAE_Dense(input_size, dense_units)\n",
    "            self.AE.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                loss='mse',\n",
    "                metrics=['mse']\n",
    "            )\n",
    "            \n",
    "        elif self.AE_type=='LSTM':\n",
    "            self.AE = DAE_LSTM(timesteps, features, LSTM_units, LSTM_dropout)\n",
    "            self.AE.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                loss='mse',\n",
    "                metrics=['mse']\n",
    "            )\n",
    "            \n",
    "        elif self.AE_type=='GRU':\n",
    "            self.AE = DAE_GRU(timesteps, features, GRU_units, GRU_dropout)\n",
    "            self.AE.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                loss='mse',\n",
    "                metrics=['mse']\n",
    "            )\n",
    "        \n",
    "        if self.prox_type=='l1':\n",
    "            self.prox_fn = prox_l1\n",
    "        elif self.prox_type=='l21':\n",
    "            self.prox_fn = prox_l21\n",
    "            \n",
    "    def train_and_fit(self, X, train_iter: int, AE_train_iter: int, batch_size: int, eps: float, lam: float, verbose=0):\n",
    "        if self.AE_type == 'Dense':\n",
    "            self.default_shape = (X.shape[0], X.shape[1])\n",
    "            self.utils_shape = (X.shape[0], X.shape[1])\n",
    "        elif self.AE_type == 'LSTM' or self.AE_type == 'GRU':\n",
    "            self.default_shape = (X.shape[0], X.shape[1], 1)\n",
    "            self.utils_shape = (X.shape[0], X.shape[1])\n",
    "        \n",
    "        X = X.reshape(self.default_shape)\n",
    "        self.L = np.zeros(self.default_shape)\n",
    "        self.S = np.zeros(self.default_shape)\n",
    "        self.LD = np.zeros(self.default_shape)\n",
    "        self.LS = X\n",
    "        \n",
    "        for i in range(train_iter):\n",
    "            if verbose!= 0:\n",
    "                print(f'RAE training iteration: {i+1}')\n",
    "            self.LD = X - self.S\n",
    "            # Now fit the autoencoder for some iters\n",
    "            self.AE.fit(x=self.LD, batch_size=batch_size, epochs=AE_train_iter, verbose=verbose)\n",
    "            self.LD = self.AE(self.LD).numpy()\n",
    "            self.S = X - self.LD\n",
    "            \n",
    "            self.S = self.S.reshape(self.utils_shape)\n",
    "            self.S = self.prox_fn(lam=lam, x=self.S.T).T\n",
    "            self.S = self.S.reshape(self.default_shape)\n",
    "            \n",
    "            c1 = tf.linalg.norm(X - self.LD - self.S) / tf.linalg.norm(X)\n",
    "            c2 = tf.linalg.norm(self.LS - self.LD - self.S) / tf.linalg.norm(X)\n",
    "            if c1 < eps or c2 < eps:\n",
    "                print(f'Early Convergence at iter {i+1}')\n",
    "                break\n",
    "            self.LS = self.LD + self.S\n",
    "        return self.LD, self.S\n",
    "    \n",
    "    def get_reconstruction(self, X):\n",
    "        return self.AE(X)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        L = X - self.S\n",
    "        return self.AE.encode(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L21 Experiment on Time series\n",
    "## Dense RDAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22464,)\n",
      "(22321, 144)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join('data', 'realKnownCause', 'machine_temperature_system_failure.csv'), delimiter=',', decimal='.')\n",
    "df.drop_duplicates(subset='timestamp', keep='first', inplace=True)\n",
    "ts_timestamps = df.iloc[:,0].values[33:-186]\n",
    "ts_values = np.array(df.iloc[:,1].values[33:-186])\n",
    "print(ts_values.shape)\n",
    "#ts_values_daily = ts_values.copy().reshape((int(ts_values.shape[0]/(12*24)), 12*24, 1))\n",
    "\n",
    "timesteps = 144\n",
    "ts_data = subsequences(ts_values, timesteps)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "ts_data_scaled = scaler.fit_transform(X=ts_data)\n",
    "\n",
    "ts_train_dense = ts_data_scaled.copy()\n",
    "np.random.shuffle(ts_train_dense)\n",
    "ts_train_GRU = ts_data_scaled.reshape((ts_data_scaled.shape[0], timesteps, 1))\n",
    "np.random.shuffle(ts_train_GRU)\n",
    "print(ts_train_dense.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAEl21GRU = RobustAutoencoder(AE_type='GRU', prox_type='l21', timesteps=timesteps, features=1, lr=3e-4, GRU_dropout=0.0, GRU_units=[32, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAE training iteration: 1\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 8s 35ms/step - loss: 0.0332 - mse: 0.0332\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0137 - mse: 0.0137\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0119 - mse: 0.0119\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0084 - mse: 0.0084\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0073 - mse: 0.0073\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0063 - mse: 0.0063\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0049 - mse: 0.0049\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0040 - mse: 0.0040\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0036 - mse: 0.0036\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0034 - mse: 0.0034\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 0.0033 - mse: 0.0033\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0033 - mse: 0.0033\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 0.0032 - mse: 0.0032\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0032 - mse: 0.0032\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 0.0032 - mse: 0.0032\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0032 - mse: 0.0032\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0032 - mse: 0.0032\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0032 - mse: 0.0032\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0032 - mse: 0.0032\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0032 - mse: 0.0032\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0031 - mse: 0.0031\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0031 - mse: 0.0031\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0031 - mse: 0.0031\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0031 - mse: 0.0031\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0031 - mse: 0.0031\n",
      "RAE training iteration: 2\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0031 - mse: 0.0031\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0031 - mse: 0.0031\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0031 - mse: 0.0031\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0031 - mse: 0.0031\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0031 - mse: 0.0031\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0031 - mse: 0.0031\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0031 - mse: 0.0031\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 0.0031 - mse: 0.0031\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0031 - mse: 0.0031\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0031 - mse: 0.0031\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0031 - mse: 0.0031\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 0.0031 - mse: 0.0031\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0031 - mse: 0.0031\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0030 - mse: 0.0030\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0030 - mse: 0.0030\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0030 - mse: 0.0030\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0028 - mse: 0.0028\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0026 - mse: 0.0026\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0021 - mse: 0.0021\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0018 - mse: 0.0018\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0017 - mse: 0.0017\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0016 - mse: 0.0016\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0016 - mse: 0.0016\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.0015 - mse: 0.0015\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0015 - mse: 0.0015\n",
      "RAE training iteration: 3\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0015 - mse: 0.0015\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 0.0015 - mse: 0.0015\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0014 - mse: 0.0014\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0014 - mse: 0.0014\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0014 - mse: 0.0014\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0014 - mse: 0.0014\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0014 - mse: 0.0014\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0014 - mse: 0.0014\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0014 - mse: 0.0014\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0013 - mse: 0.0013\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0013 - mse: 0.0013\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0013 - mse: 0.0013\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 0.0013 - mse: 0.0013\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0013 - mse: 0.0013\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0013 - mse: 0.0013\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0013 - mse: 0.0013\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0013 - mse: 0.0013\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0013 - mse: 0.0013\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0012 - mse: 0.0012\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0012 - mse: 0.0012\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0012 - mse: 0.0012\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 0.0012 - mse: 0.0012\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.0012 - mse: 0.0012\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 0.0012 - mse: 0.0012\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0012 - mse: 0.0012\n",
      "RAE training iteration: 4\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0012 - mse: 0.0012\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0012 - mse: 0.0012\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0012 - mse: 0.0012\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0012 - mse: 0.0012\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0012 - mse: 0.0012\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0011 - mse: 0.0011\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0010 - mse: 0.0010\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.0010 - mse: 0.0010\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0010 - mse: 0.0010\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 0.0010 - mse: 0.0010\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0010 - mse: 0.0010\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 9.9996e-04 - mse: 9.9996e-04\n",
      "RAE training iteration: 5\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 9.9200e-04 - mse: 9.9200e-04\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 9.7456e-04 - mse: 9.7456e-04\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 9.5992e-04 - mse: 9.5992e-04\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 9.5139e-04 - mse: 9.5139e-04\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 9.4409e-04 - mse: 9.4409e-04\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 9.1472e-04 - mse: 9.1472e-04\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 8.8803e-04 - mse: 8.8803e-04\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 8.6582e-04 - mse: 8.6582e-04\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 8.3378e-04 - mse: 8.3378e-04\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 7.9813e-04 - mse: 7.9813e-04\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 7.8486e-04 - mse: 7.8486e-04\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 7.7135e-04 - mse: 7.7135e-04\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 7.5406e-04 - mse: 7.5406e-04\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 7.4117e-04 - mse: 7.4118e-04\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 7.1788e-04 - mse: 7.1788e-04\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 7.0597e-04 - mse: 7.0598e-04\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 7.0459e-04 - mse: 7.0459e-04\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 6.9138e-04 - mse: 6.9138e-04\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 6.8807e-04 - mse: 6.8807e-04\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 6.9163e-04 - mse: 6.9163e-04\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 6.7152e-04 - mse: 6.7152e-04\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 6.6672e-04 - mse: 6.6672e-04\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 6.7273e-04 - mse: 6.7273e-04\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 6.5397e-04 - mse: 6.5397e-04\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 6.5092e-04 - mse: 6.5092e-04\n",
      "RAE training iteration: 6\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 6.4886e-04 - mse: 6.4886e-04\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 6.3526e-04 - mse: 6.3526e-04\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 6.4388e-04 - mse: 6.4388e-04\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 6.2941e-04 - mse: 6.2941e-04\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 6.3368e-04 - mse: 6.3368e-04\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 6.2301e-04 - mse: 6.2301e-04\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 6.2372e-04 - mse: 6.2372e-04\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 6.2188e-04 - mse: 6.2188e-04\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 6.1030e-04 - mse: 6.1030e-04\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 6.0379e-04 - mse: 6.0379e-04\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 6.0372e-04 - mse: 6.0372e-04\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 5.9638e-04 - mse: 5.9638e-04\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 5.9406e-04 - mse: 5.9406e-04\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 5.9623e-04 - mse: 5.9623e-04\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 5.8470e-04 - mse: 5.8470e-04\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 5.8261e-04 - mse: 5.8261e-04\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 5.7775e-04 - mse: 5.7775e-04\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 5.7188e-04 - mse: 5.7188e-04\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 5.7739e-04 - mse: 5.7739e-04\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 5.5837e-04 - mse: 5.5837e-04\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 5.5924e-04 - mse: 5.5924e-04\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 5.5433e-04 - mse: 5.5433e-04\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 5.7993e-04 - mse: 5.7993e-04\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 5.4992e-04 - mse: 5.4992e-04\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 5.4360e-04 - mse: 5.4360e-04\n",
      "RAE training iteration: 7\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 5.3652e-04 - mse: 5.3652e-04\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 5.2897e-04 - mse: 5.2897e-04\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 5.3933e-04 - mse: 5.3933e-04\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 5.2702e-04 - mse: 5.2702e-04\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 5.2418e-04 - mse: 5.2418e-04\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 5.1750e-04 - mse: 5.1750e-04\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 5.8614e-04 - mse: 5.8614e-04\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 5.1119e-04 - mse: 5.1119e-04\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 5.0653e-04 - mse: 5.0653e-04\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 5.0261e-04 - mse: 5.0261e-04\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 5.0102e-04 - mse: 5.0102e-04\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 5.0338e-04 - mse: 5.0338e-04\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 4.9283e-04 - mse: 4.9283e-04\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.9266e-04 - mse: 4.9266e-04\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.9149e-04 - mse: 4.9149e-04\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.9811e-04 - mse: 4.9811e-04\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.9365e-04 - mse: 4.9365e-04\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.8274e-04 - mse: 4.8274e-04\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.8457e-04 - mse: 4.8457e-04\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.8519e-04 - mse: 4.8519e-04\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.8158e-04 - mse: 4.8158e-04\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 4.7470e-04 - mse: 4.7470e-04\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 4.7833e-04 - mse: 4.7833e-04\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 4.7349e-04 - mse: 4.7349e-04\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.6602e-04 - mse: 4.6602e-04\n",
      "RAE training iteration: 8\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.6534e-04 - mse: 4.6534e-04\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.6293e-04 - mse: 4.6293e-04\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.6571e-04 - mse: 4.6571e-04\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.6707e-04 - mse: 4.6707e-04\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.6970e-04 - mse: 4.6970e-04\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.5940e-04 - mse: 4.5940e-04\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.7589e-04 - mse: 4.7589e-04\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.5756e-04 - mse: 4.5756e-04\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.6100e-04 - mse: 4.6100e-04\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 4.5053e-04 - mse: 4.5053e-04\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.5785e-04 - mse: 4.5785e-04\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.4620e-04 - mse: 4.4620e-04\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.4759e-04 - mse: 4.4759e-04\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 4.4823e-04 - mse: 4.4823e-04\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.4216e-04 - mse: 4.4216e-04\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.5209e-04 - mse: 4.5209e-04\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.5096e-04 - mse: 4.5096e-04\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.4178e-04 - mse: 4.4178e-04\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 4.3990e-04 - mse: 4.3990e-04\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.4016e-04 - mse: 4.4016e-04\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 4.3627e-04 - mse: 4.3627e-04\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.5231e-04 - mse: 4.5231e-04\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.3678e-04 - mse: 4.3678e-04\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.3455e-04 - mse: 4.3455e-04\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.3111e-04 - mse: 4.3111e-04\n",
      "RAE training iteration: 9\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.3090e-04 - mse: 4.3090e-04\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 4.3595e-04 - mse: 4.3595e-04\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.2696e-04 - mse: 4.2696e-04\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.2788e-04 - mse: 4.2788e-04\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.3322e-04 - mse: 4.3322e-04\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.2843e-04 - mse: 4.2843e-04\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.6988e-04 - mse: 4.6988e-04\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 4.2372e-04 - mse: 4.2372e-04\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.1835e-04 - mse: 4.1835e-04\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.1630e-04 - mse: 4.1630e-04\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.2585e-04 - mse: 4.2585e-04\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.2085e-04 - mse: 4.2085e-04\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.2377e-04 - mse: 4.2377e-04\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.2401e-04 - mse: 4.2401e-04\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.1439e-04 - mse: 4.1439e-04\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.1730e-04 - mse: 4.1730e-04\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.1495e-04 - mse: 4.1495e-04\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.2205e-04 - mse: 4.2205e-04\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.0965e-04 - mse: 4.0965e-04\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.1049e-04 - mse: 4.1049e-04\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.1026e-04 - mse: 4.1026e-04\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 4.1199e-04 - mse: 4.1199e-04\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.1319e-04 - mse: 4.1319e-04\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 4.0798e-04 - mse: 4.0798e-04\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.0546e-04 - mse: 4.0546e-04\n",
      "RAE training iteration: 10\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.0305e-04 - mse: 4.0305e-04\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.0707e-04 - mse: 4.0707e-04\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.0691e-04 - mse: 4.0691e-04\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.0561e-04 - mse: 4.0561e-04\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 3.9968e-04 - mse: 3.9968e-04\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.0023e-04 - mse: 4.0023e-04\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.0643e-04 - mse: 4.0643e-04\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.0829e-04 - mse: 4.0829e-04\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 3.9809e-04 - mse: 3.9809e-04\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 4.0493e-04 - mse: 4.0493e-04\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.0371e-04 - mse: 4.0371e-04\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 3.9595e-04 - mse: 3.9595e-04\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 4.0152e-04 - mse: 4.0152e-04\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 3.9280e-04 - mse: 3.9280e-04\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 3.9024e-04 - mse: 3.9024e-04\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 3.9327e-04 - mse: 3.9327e-04\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 3.9705e-04 - mse: 3.9705e-04\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 3.9489e-04 - mse: 3.9489e-04\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 3.8658e-04 - mse: 3.8658e-04\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 3.8665e-04 - mse: 3.8665e-04\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 3.8830e-04 - mse: 3.8830e-04\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 3.8845e-04 - mse: 3.8845e-04\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 3.9229e-04 - mse: 3.9229e-04\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 3.8740e-04 - mse: 3.8740e-04\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 3.8640e-04 - mse: 3.8640e-04\n"
     ]
    }
   ],
   "source": [
    "LD_l21_GRU, S_l21_GRU = RAEl21GRU.train_and_fit(X=ts_train_GRU, AE_train_iter=25, train_iter=10, lam=lam, batch_size=256, eps=1e-10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "Detected anomalies: 0\n"
     ]
    }
   ],
   "source": [
    "from pyexpat import features\n",
    "from time import time\n",
    "\n",
    "\n",
    "shrinked_shape = (ts_train_GRU.shape[0], ts_train_GRU.shape[1])\n",
    "\n",
    "detected_anomalies_GRU = (np.linalg.norm(S_l21_GRU.reshape(shrinked_shape), axis=1) > 0.).astype(int)\n",
    "print(detected_anomalies_GRU)\n",
    "print(f'Detected anomalies: {np.sum(detected_anomalies_GRU)}')\n",
    "\n",
    "shrinked_shape = (ts_train_GRU.shape[0], ts_train_GRU.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "Detected anomalies: 0\n"
     ]
    }
   ],
   "source": [
    "detected_anomalies_GRU = (np.linalg.norm(S_l21_GRU.reshape(shrinked_shape), axis=1) > 0.).astype(int)\n",
    "print(detected_anomalies_GRU)\n",
    "print(f'Detected anomalies: {np.sum(detected_anomalies_GRU)}')\n",
    "\n",
    "non_anomalies = np.argwhere(1-detected_anomalies_GRU)\n",
    "np.random.shuffle(non_anomalies)\n",
    "\n",
    "anomalies = np.argwhere(detected_anomalies_GRU)\n",
    "np.random.shuffle(anomalies)\n",
    "\n",
    "with open(os.path.join('l21_experiment_ts', 'result'+str(lam)+'.txt'), 'a') as f:\n",
    "    print('GRU stats:', file=f)\n",
    "    print(f'Anomalies: {len(anomalies)}', file=f)\n",
    "    print(f'Non anomalies: {len(non_anomalies)}', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "shrinked_shape = (ts_train_GRU.shape[0], ts_train_GRU.shape[1])\n",
    "\n",
    "if len(non_anomalies)>0:\n",
    "    for i in range(min(len(non_anomalies), 10)):\n",
    "        ind = non_anomalies[i][0]\n",
    "        recon = RAEl21GRU.get_reconstruction(ts_train_GRU[ind].reshape((1,timesteps,1)).reshape((1,timesteps)))\n",
    "        save_ts_recon(ts_train_GRU[ind].reshape(timesteps),\n",
    "                recon.numpy().reshape((timesteps)),\n",
    "                LD_l21_GRU[ind],\n",
    "                S_l21_GRU[ind],\n",
    "                ind+1,\n",
    "                zoom=True,\n",
    "                filename=os.path.join('l21_experiment_ts', 'GRUlam'+str(lam)+'ts_non_anomaly'+str(ind+1)+'.jpg')\n",
    "        )\n",
    "        \n",
    "if len(non_anomalies)>0:\n",
    "    for i in range(min(len(non_anomalies), 10)):\n",
    "        ind = non_anomalies[i][0]\n",
    "        recon = RAEl21GRU.get_reconstruction(ts_train_GRU[ind].reshape((1,timesteps,1)).reshape((1,timesteps)))\n",
    "        save_ts_recon(ts_train_GRU[ind].reshape(timesteps),\n",
    "                recon.numpy().reshape((timesteps)),\n",
    "                LD_l21_GRU[ind],\n",
    "                S_l21_GRU[ind],\n",
    "                ind+1,\n",
    "                zoom=True,\n",
    "                filename=os.path.join('l21_experiment_ts', 'GRUlam'+str(lam)+'ts_non_anomalyzoom'+str(ind+1)+'.jpg')\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(anomalies)>0:\n",
    "    for i in range(min(len(anomalies), 10)):\n",
    "        ind = anomalies[i][0]\n",
    "        recon = RAEl21GRU.get_reconstruction(ts_train_GRU[ind].reshape((1,timesteps,1)).reshape((1,timesteps)))\n",
    "        save_ts_recon(ts_train_GRU[ind].reshape(timesteps),\n",
    "                recon.numpy().reshape((timesteps)),\n",
    "                LD_l21_GRU[ind],\n",
    "                S_l21_GRU[ind],\n",
    "                ind+1,\n",
    "                zoom=False,\n",
    "                filename=os.path.join('l21_experiment_ts', 'GRUlam'+str(lam)+'ts_anomaly'+str(ind+1)+'.jpg')\n",
    "        )\n",
    "        \n",
    "if len(anomalies)>0:\n",
    "    for i in range(min(len(anomalies), 10)):\n",
    "        ind = anomalies[i][0]\n",
    "        recon = RAEl21GRU.get_reconstruction(ts_train_GRU[ind].reshape((1,timesteps,1)).reshape((1,timesteps)))\n",
    "        save_ts_recon(ts_train_GRU[ind].reshape(timesteps),\n",
    "                recon.numpy().reshape((timesteps)),\n",
    "                LD_l21_GRU[ind],\n",
    "                S_l21_GRU[ind],\n",
    "                ind+1,\n",
    "                zoom=True,\n",
    "                filename=os.path.join('l21_experiment_ts', 'GRUlam'+str(lam)+'ts_anomalyzoom'+str(ind+1)+'.jpg')\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2692c985375d31b3da8770a087597fbdbb725d2d8a937843e14c78467e5dd825"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
