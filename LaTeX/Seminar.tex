%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass{beamer}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
%\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
\usetheme{Warsaw}
%\beamertemplatenavigationsymbolsempty

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{etex}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{cite}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{mathrsfs,mathtools}
\usepackage{graphicx}
\usepackage{float}

\usepackage{hyperref} % References become hyperlinks.
\hypersetup{
	%colorlinks = true,
	linkcolor = {blue},
	urlcolor = {red},
	citecolor = {blue},
	%pdfenconing=auto,
}
\usepackage{wrapfig}
%\usepackage{arydshln}
\usepackage{array}
\usepackage[T1]{fontenc} 
\usepackage{bm}
\usepackage{multicol, multirow}
\usepackage{grffile,pgf,tikz}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{animate}
\usepackage{caption}
\usepackage{subcaption}


% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal

% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\usepackage{listings}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
		language=Python,
		basicstyle=\ttm,
		morekeywords={self},              % Add keywords here
		keywordstyle=\ttb\color{deepblue},
		emph={MyClass,__init__},          % Custom highlighting
		emphstyle=\ttb\color{deepred},    % Custom highlighting style
		stringstyle=\color{deepgreen},
		frame=tb,                         % Any extra options here
		showstringspaces=false
}}


% Python environment
\lstnewenvironment{python}[1][]
{
	\pythonstyle
	\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
		\pythonstyle
		\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}



\usetikzlibrary{matrix}
\usetikzlibrary{shapes.geometric,calc,arrows}

%\usepackage{unicode-math}
%\setmathfont{XITS Math}
%\setmathfont[version=setB,StylisticSet=1]{XITS Math}


\theoremstyle{plain}
\newtheorem{teo}{Teorema}
%\newtheorem{lemma}[teo]{Lemma}
\newtheorem{prop}[teo]{Proposizione}
\newtheorem{post}{Postulato}
\newtheorem{cor}[teo]{Corollario}


\theoremstyle{definition}
\newtheorem{defn}{Definizione}
\newtheorem{exmp}[defn]{Esempio}
\newtheorem{oss}[defn]{Osservazione}
\newtheorem{prob}{Problema}
\newtheorem*{prob*}{Problema}
\newtheorem{hint}{Indizio}
\newtheorem*{notaz}{Notazione}

\theoremstyle{remark}
\newtheorem*{rem}{Remark}


\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\M}{\mathbb{M}}
\newcommand{\LL}{\mathscr{L}}
\newcommand{\HH}{\mathbb{H}}
\newcommand{\SP}{\mathbb{S}}
\newcommand{\dsum}{\displaystyle\sum}
\newcommand{\dint}{\displaystyle\int}
\newcommand{\scal}[2]{\langle #1,#2 \rangle}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\eval}[3]{\Big[ #1 \Big]_{#2}^{#3}}
%\newcommand{\sob}[3]{W^{#1, #2}(#3)}
%\newcommand{\sobzero}[3]{W_{0}^{#1, #2}(#3)}
%\newcommand{\sobloc}[3]{W_{\text{loc}}^{#1, #2}(#3)}
\newcommand{\weakconv}{\rightharpoonup}
\newcommand{\weakconvs}{\overset{\ast}{\rightharpoonup}}
\newcommand{\prox}{\text{prox}}


\newcommand{\dx}{\text{d}x}
\newcommand{\dt}{\text{d}t}
\newcommand{\dy}{\text{d}y}
\newcommand{\diff}{\text{d}}
\newcommand{\dX}{\text{d}\bm{x}}
\newcommand{\dFX}{\text{d}F(\bm{x})}
\newcommand{\dfX}{\text{d}f(\bm{x})}
\newcommand{\dFx}{\text{d}F(x)}
\newcommand{\dfx}{\text{d}f(x)}
\newcommand{\X}{\bm{X}}
\newcommand{\x}{\bm{x}}
\newcommand{\B}{\bm{b}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\Pro}{\mathbf{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\bh}{\hat{\bm{b}}}
\newcommand{\Sh}{\hat{S}}
\newcommand{\dmu}{\text{d}\mu(\B)}
\newcommand{\Ph}{\hat{\mathbf{P}}}
\newcommand{\Ct}{\tilde{C}}
\newcommand{\Dt}{\tilde{D}}


\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Orb}{Orb}
\DeclareMathOperator{\Stab}{Stab}
\DeclareMathOperator{\Fix}{Fix}
\DeclareMathOperator{\Ind}{Ind}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Imm}{Im}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\Res}{Res}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\Char}{char}
\DeclareMathOperator{\cof}{cof}
\DeclareMathOperator{\rk}{rk}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\dive}{div}
\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\Lip}{Lip}
\DeclareMathOperator{\diam}{diam}
\DeclareMathOperator{\Int}{int}
\DeclareMathOperator{\extr}{extr}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\divergence}{div}
\DeclareMathOperator{\baric}{bar}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\grad}{\nabla}
\newcommand{\perim}{\mathcal{P}}%%%%%%%%%%%%%%%
%%ATTENZIONE: HO TOLTO LE PARENTESI, ORA \perim METTE SOLO LA P
\newcommand{\symmdiff}{\Delta}
\newcommand{\bdry}{\partial}
\newcommand{\clos}[1]{\overline{#1}}
\newcommand{\lebesgue}{\ensuremath{\mathscr{L}}}


%definizioni che servono solo per la tesi magistrale
\newcommand{\FF}{\mathcal{F}}	%funzionale completo dell'energia
\newcommand{\FFF}{\widetilde{\mathcal{F}}} %funzionale dell'energia sui sottoinsiemi disgiunti
\newcommand{\GG}{\mathcal{G}}	%funzionale con potenza positiva al posto del perimetro
\newcommand{\RR}{\mathcal{R}}	%funzionale di Riesz
\newcommand{\riesz}[1]{\iint_{#1 \times #1}\frac{1}{|x-y|^{N-\alpha}}\dx\dy}
\newcommand{\genriesz}[2]{\iint_{#1 \times #1}#2(|x-y|)\dx\dy}



%debug
\newcommand{\avviso}[1]{{\textcolor{red}{\textbf{#1}}}}


\newcommand\restr[2]{\ensuremath{\left.#1\right|_{#2}}}


\newcommand{\boh}{\textcolor{red}{\Huge\textbf{???}}}
\newcommand{\attenzione}{\textcolor{red}{\Huge\textbf{!!!}}}
\newcommand{\vitali}{\textcolor{red}{\Huge\textbf{Vitali}}}




%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Anomaly Detection with Robust Deep Autoencoders]{Anomaly Detection with Robust Deep Autoencoders} % The short title appears at the bottom of every slide, the full title is only on the title page
\subtitle[]{original article by C. Zhou and R. C. Peffenroth}
\author[Alessandro Trenta]{\emph{Alessandro Trenta}} % Your name
\institute[SNS] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{Scuola Normale Superiore \\ % Your institution for the title page
% Your email address
}
\date{} % Date, can be changed to a custom date

\begin{document}

\begin{frame}
	\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
	\tableofcontents
\end{frame}

\nocite{RAE}

\section{Background}

\begin{frame}
	\frametitle{Deep Autoencoders}
	\begin{itemize}
		\item A Deep Autoencoder (DAE) is constituted by two main components: an encoder $E$ and a Decoder $D$.
		\item The main objective of a DAE is to learn the identity map so that the reconstruction $\bar{X}=D(E(X))$ is as close as possible to the original input $X$.
		\item The encoder and decoder functions $E, D$ can be any kind of mapping between the data space and the coding space. Usually they are Deep Neural Networks e.g. a feed forward network or even more complex models such as Long ShortTer Memory (LSTM).
		\item The objective is usually to find the minimum reconstruction error w.r.t. some parametrized encoding and decoding functions and a distance (in this case the $L_2$ norm)
			\begin{equation}
				\min_{\theta, \phi}{\norm{X-D_{\theta}(E_{\phi}(X))}_{2}}
			\end{equation}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Principal Component Analysis}
	\begin{itemize}
		\item Assume to have a set of $N$ samples of $n$ dimensional data, so that $X\in \R^{N\times n}$ s.t. each column has $0$ mean (we can just shift the data to fulfill this request).
		\item Principal Component Analysis (PCA) is defined as an orthogonal linear transformation such that the new coordinate system of $\R^{n}$ satisfies: 
			the $i$-th component of the coordinate system has the $i$-th greatest data variance if we project all samples on that component.
		\item Ideally we are trying to fit a $n$-ellipsoid into the data. The length of an axis of the ellipsoid represents the variance of data along that axis.
		\item PCA is often used for dimensionality reduction or encoding: we can project the data on the first $k<n$ principal components.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Principal Component Analysis}
	Mathematically we can define:
		\begin{equation}
			w_1 = \argmax_{\norm{w}_{2}=1}{\norm{Xw}_{2}^2} = \argmax_{w}{\frac{w^TX^TXw}{w^Tw}}
		\end{equation}
		for the first component. Then for the $k$-th component we first subtract the first $k-1$ principal component from $X$
		\begin{equation}
			\hat{X}_{k} = X - \sum_{i=1}^{k-1}{Xw_{i}w_{i}^T}
		\end{equation}
		and finally solving again the similar problem:
		\begin{equation}
			w_k = \argmax_{\norm{w}_{2}=1}{\norm{\hat{X}_{k}w}_{2}^2} = \argmax_{w}{\frac{w^T\hat{X}_{k}^T\hat{X}_{k}w}{w^Tw}}
		\end{equation}
\end{frame}

\begin{frame}
	\frametitle{Robust Principal Component Analysis}
	\begin{itemize}
		\item Robust Principal Component Analysis (RPCA) is a generalization of PCA that aims to reduce the sensitivity of PCA to outliers.
		\item The idea is to find a low-dimensional representation of data cleaned from the sparse outliers that can disturb the PCA process.
		\item We therefore assume that data $X$ can be represented as $X = L + S$: $L$ has low rank and is the low-dimensional representation of $X$ while $S$ is a sparse matrix 
			consisting of the outlier elements that cannot be captured by the representation.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Robust Principal Component Analysis}
	\begin{itemize}
		\item The problem can be addressed as:
			\begin{align}
				\min_{L,S}{\rho(L)+\lambda \norm{S}_{0}}\\
				\text{s. t. }\norm{X - L - S}_{F}^2=0 
			\end{align}
			where $\rho(\cdot)$ is the rank of a matrix and we used the zero norm.
		\item This optimization problem is NP-hard and tractable only for small metrices.
		\item Usually it is substituted by the following problem, which is convex and tractable also for large matrices:
			\begin{align}
				\min_{L,S}{\norm{L}_{*}+\lambda \norm{S}_{1}}\\
				\text{s. t. }\norm{X - L - S}_F^2=0
			\end{align}
			where $\norm{\cdot}_*$ is the nuclear norm i. e. the sum of singular values of a matrix.
	\end{itemize}
\end{frame}

\section{Robust Deep Autoencoders}

\begin{frame}
	\frametitle{Robust Deep Autoencoders}
	\begin{itemize}
		\item The main idea behind Robust Deep Autoencoders (RDAE) is to combine the representation learning of DAEs and the anomaly detection capability of RPCA.
		\item Noise and outliers are incompressible in the lower dimensional space we want to represent our data in.
		\item The objective is to learn a good low dimensional representation except for few exceptions.
		\item We will see two RDAE typed, one for $l_1$ regularization and one for $l_{2,1}$.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{RDAE with $l_1$ regularization}
	\begin{itemize}
		\item The RDAE objective is to decompose data $X=L_D+S$ just as in RPCA.
		\item By removing the noise $S$ the autoencoder can better reconstruct $L_D$.
		\item As before, the best choice to obtain a sparse $S$ would be to use a loss of the type $\norm{S}_0$ which counts the non-zero entries, solving the problem
			\begin{align}
				\min_{\theta}{\norm{L_D -D_{\theta}(E_{\theta}(L_D))}_2 + \lambda\norm{S}_0}\\
				\text{s.t. }X-L_D-S=0  
			\end{align}
		\item The parameter $\lambda$ controls the sparsity of $S$ and plays an essential role.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{The role of $\lambda$}
	\begin{itemize}
		\item As we said, the role of $\lambda$ is very important.
		\item A smaller $\lambda$ means that the norm of $S$ plays a less important role and much of the loss will come from the DAE.
		\item The model will reconstruct better but recognize less outliers. This could be helpful if we want a more faithful representation e.g. for supervised tasks.
		\item A larger $\lambda$, instead, gives more importance to the norm of $S$ as a loss.
		\item This means that the model will recognize more (or even too much) outliers, sacrificing some reconstruction performance.
	\end{itemize}
\end{frame}

	
\begin{frame}
	\frametitle{The true objective}
	\begin{itemize}
		\item As for the RPCA the previous loss is non tractable. We then instead focus on the following problem:
			\begin{align}
				\min_{\theta}{\norm{L_D -D_{\theta}(E_{\theta}(L_D))}_2 + \lambda\norm{S}_1}\\
				\text{s.t. }X-L_D-S=0  
			\end{align}
			Notice two things:
		\item The autoencoder is trained with $L_D$, the part of its decomposition assumed to be outliers and noise free.
		\item There is no specific requirement about the DAE, in fact $D_{\theta}$ and $E_{\theta}$ are generic decoder, encoder functions.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{RDAE with $l_{2,1}$ regularization}
	\begin{itemize}
		\item The RDAE with $l_1$ penalization assumes that outliers and noise are not structured. The $l_1$ penalty is indeed just a regularization to induce sparsity.
		\item In general we can have multiple types of errors: an instrument that corrupts an input feature or outliers where the input data is in some way structurally different from normal data.
	\end{itemize}
\end{frame}

\begin{frame}
	\centering
	\includegraphics[width=0.9\linewidth]{Images/diff_anomalies.png}
\end{frame}

\begin{frame}
	\frametitle{The $l_{2,1}$ norm}
	\begin{itemize}
		\item The $l_{2,1}$ norm is defined as ($X\in \R^{N\times n}$):
			\begin{equation}
				\norm{X}_{2,1} = \sum_{j=1}^{n}{\norm{X_j}_{2}} = \sum_{j=1}^{n}{\left(\sum_{i=1}^{N}|X_{ij}|^{2}\right)^{\frac{1}{2}}}
			\end{equation}
		\item The $l_{2,1}$ norm can be seen as introducting a $l_2$ norm regularization over each feature and then adding a $l_1$ regularization accross features.
		\item We can also do the other way around: to recognize data anomalies (by row) just apply the $l_{2,1}$ norm to $X^T$.
	\end{itemize}
\end{frame}

\begin{frame}
	\begin{itemize}
		\item The final optimization problem for the RDAE with $l_{2,1}$ regularization for data anomalies is then
			\begin{align}
				\min_{\theta}{\norm{L_D -D_{\theta}(E_{\theta}(L_D))}_2 + \lambda\norm{S^T}_{2,1}}\\
				\text{s.t. }X-L_D-S=0  
			\end{align}
		\item For detecting feature anomalies we just need to change the objective to
			\begin{align}
				\min_{\theta}{\norm{L_D -D_{\theta}(E_{\theta}(L_D))}_2 + \lambda\norm{S}_{2,1}}\\
				\text{s.t. }X-L_D-S=0  
			\end{align}
	\end{itemize}
\end{frame}

\section{RDAE training}

\begin{frame}
	\frametitle{The proximal operator}
	\begin{itemize}
		\item To see in detail the training procedure for the RDAE we first need to consider the proximal operator.
		\item For general optimization problems of the form $\min f(x)+\lambda g(x)$ where $g$ is convex some of the most used methods require to find
			\begin{equation}
				\prox_{\lambda, g}(x) = \argmin_{y}{g(y)+\frac{1}{2\lambda}\norm{x-y}_{2}^{2}}
			\end{equation}
		\item In this case we then want to obtain a solution of the problems
			\begin{align}
				\prox_{\lambda, l_1}(x) = \argmin_{y}{l_1(y)+\frac{1}{2\lambda}\norm{x-y}_{2}^{2}}\\
				\prox_{\lambda, l_{2,1}}(x) = \argmin_{y}{l_{2,1}(y)+\frac{1}{2\lambda}\norm{x-y}_{2}^{2}}
			\end{align}
	\end{itemize}
\end{frame}

\begin{frame}
	\begin{itemize}
		\item For the $l_1$ norm, the solution to the proximal problem is
			\begin{equation}
				\prox_{\lambda, l_1}(x) = \begin{cases}
					x_i - \lambda, & x_i>\lambda\\
					x_i + \lambda, & x_i<-\lambda\\
					0, & x_i\in[-\lambda, \lambda]
				\end{cases}
			\end{equation}
			which in the case of $S\in\R^{N\times n}$ gets applied element by element.
		\item For the $l_{2,1}$ norm, we obtain (let $S_{\cdot j}$ be the column vector $S_{ij}, j=1, \ldots, N$)
			\begin{equation}
				(\prox_{\lambda, l_{2,1}}(S))_{ij} = \begin{cases}
					S_{ij}-S_{ij} - \lambda\frac{S_{ij}}{\norm{S_{\cdot j}}_{2}}, & \norm{S_{\cdot j}}_{2} > \lambda\\
					0, & \norm{S_{\cdot j}}_{2} \leq \lambda
				\end{cases}
			\end{equation}
			if we are considering feature wise anomalies, substitute $S$ with $S^T$ for data anomalies.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{The main algorithm}
	\begin{itemize}
		\item The method used to train the RDAE is the Alternating Direction Method of Multipliers (ADMM).
		\item The main idea is to optimize the problem
			\begin{align}
				\min_{\theta}{\norm{L_D -D_{\theta}(E_{\theta}(L_D))}_2 + \lambda\norm{S^T}_{2,1}}\\
				\text{s.t. }X-L_D-S=0  
			\end{align}
			by doing it in two steps at each iteration.
		\item First, we fix $S$ and optimize the DAE loss $\norm{L_D -D_{\theta}(E_{\theta}(L_D))}_2$ with backpropagation as usual.
		\item Then, we fix $L_D$ and optimize the regularization term with the proximal method.
	\end{itemize}
\end{frame}

\begin{frame}
	The full procedure is the following: given input $X\in \R^{N\times n}$, initialize $L_D\in \R^{N\times n}, S\in \R^{N\times n}$ as zero matrices, 
	$L_S = X$ and initialize the DAE randomly. For each iteration do:
	\begin{itemize}
		\item $L_D = X - S$
		\item Minimize $\norm{L_D -D_{\theta}(E_{\theta}(L_D))}_2$ with backpropagation.
		\item Set $L_D = D(E(L_D))$ as the reconstruction.
		\item Set $S = X - L_D$.
		\item Optimize $S$ using a $\prox_{\lambda, l_{\cdot}}$ function of choice.
		\item If $c_1 = \frac{\norm{X-L_D-S}_2}{\norm{X}_2} < \epsilon$ or $sc_2 = \frac{\norm{LS-L_D-S}_2}{\norm{X}_2} < \epsilon$ we have early convergence.
		\item Set $L_S = L_D + S$.
	\end{itemize}
	Return $L_D$ and $S$.
\end{frame}

\begin{frame}
\centering{\hyperref{https://github.com/AlexThirty/SaMLMfTSA}{}{}{https://github.com/AlexThirty/SaMLMfTSA}}
	\Huge{\centerline{Thank you!}} 
\end{frame}

\nocite{RAE}
\nocite{DATA}
\nocite{RPCA}
\nocite{PROX}

\bibliographystyle{alpha}
\bibliography{Bibliography}



%----------------------------------------------------------------------------------------

\end{document}
