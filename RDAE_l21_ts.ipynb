{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import PIL.Image as Image\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import importlib\n",
    "from plot_utils.MNIST_plot_utils import scale_to_unit_interval, save_ten_images, plot_ten_images, tile_raster_images\n",
    "from plot_utils.ts_plot_utils import plot_ts, plot_ts_recon, save_ts, save_ts_recon\n",
    "from plot_utils.heatmap import heatmap, annotate_heatmap\n",
    "\n",
    "seed=30\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequences(ts, window):\n",
    "    shape = (ts.size - window + 1, window)\n",
    "    strides = ts.strides * 2\n",
    "    return np.lib.stride_tricks.as_strided(ts, shape=shape, strides=strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox_l1(lam, x):\n",
    "    return (x > lam) * (x - lam) + (x < -lam) * (x + lam)\n",
    "\n",
    "def prox_l21(lam, x):\n",
    "    e = np.linalg.norm(x, axis=0, keepdims=False)\n",
    "    for i in range(len(e)):\n",
    "        if e[i] > lam:\n",
    "            x[:,i] = x[:,i] - lam*e[i]\n",
    "        else:\n",
    "            x[:,i] = np.zeros(len(x[:,i]))\n",
    "    return x\n",
    "    \n",
    "\n",
    "def get_Dense_encoder(input_size, dense_units):\n",
    "    encoder = tf.keras.Sequential()\n",
    "    encoder.add(layers.Input(shape=(input_size)))\n",
    "    for i in range(len(dense_units)):\n",
    "        encoder.add(layers.Dense(units=dense_units[i], activation='relu'))\n",
    "    return encoder\n",
    "\n",
    "def get_Dense_decoder(input_size, dense_units):\n",
    "    decoder = tf.keras.Sequential()\n",
    "    decoder.add(layers.Input(shape=(dense_units[-1])))\n",
    "    for i in reversed(range(len(dense_units)-1)):\n",
    "        decoder.add(layers.Dense(units=dense_units[i], activation='relu'))\n",
    "    decoder.add(layers.Dense(units=input_size, activation='sigmoid'))\n",
    "    return decoder\n",
    "\n",
    "def get_LSTM_encoder(timesteps, features, LSTM_units, LSTM_dropout):\n",
    "    encoder = tf.keras.Sequential()\n",
    "    if len(LSTM_units) > 0:\n",
    "        encoder.add(layers.LSTM(units=LSTM_units[0], dropout=LSTM_dropout, return_sequences=True, input_shape=(timesteps, features)))\n",
    "        for i in range(len(LSTM_units)-2):\n",
    "            encoder.add(layers.LSTM(units=LSTM_units[i+1], dropout=LSTM_dropout, return_sequences=True))\n",
    "        encoder.add(layers.LSTM(units=LSTM_units[-1], dropout=LSTM_dropout, return_sequences=False))\n",
    "    else:\n",
    "        encoder.add(layers.LSTM(units=LSTM_units[0], dropout=LSTM_dropout, return_sequences=False, input_shape=(timesteps, features)))            \n",
    "    return encoder\n",
    "\n",
    "def get_LSTM_decoder(timesteps, features, LSTM_units, LSTM_dropout):\n",
    "    decoder = tf.keras.Sequential()\n",
    "    decoder.add(layers.RepeatVector(timesteps))\n",
    "    for i in reversed(range(len(LSTM_units))):\n",
    "        decoder.add(layers.LSTM(units=LSTM_units[i], dropout=LSTM_dropout, return_sequences=True))\n",
    "    decoder.add(layers.TimeDistributed(layer=layers.Dense(units=features, activation='sigmoid')))\n",
    "    return decoder\n",
    "\n",
    "\n",
    "def get_GRU_encoder(timesteps, features, GRU_units, GRU_dropout):\n",
    "    encoder = tf.keras.Sequential()\n",
    "    if len(GRU_units) > 0:\n",
    "        encoder.add(layers.GRU(units=GRU_units[0], dropout=GRU_dropout, return_sequences=True, input_shape=(timesteps, features)))\n",
    "        for i in range(len(GRU_units)-2):\n",
    "            encoder.add(layers.GRU(units=GRU_units[i+1], dropout=GRU_dropout, return_sequences=True))\n",
    "        encoder.add(layers.GRU(units=GRU_units[-1], dropout=GRU_dropout, return_sequences=False))\n",
    "    else:\n",
    "        encoder.add(layers.GRU(units=GRU_units[0], dropout=GRU_dropout, return_sequences=False, input_shape=(timesteps, features)))            \n",
    "    return encoder\n",
    "\n",
    "def get_GRU_decoder(timesteps, features, GRU_units, GRU_dropout):\n",
    "    decoder = tf.keras.Sequential()\n",
    "    decoder.add(layers.RepeatVector(timesteps))\n",
    "    for i in reversed(range(len(GRU_units))):\n",
    "        decoder.add(layers.GRU(units=GRU_units[i], dropout=GRU_dropout, return_sequences=True))\n",
    "    decoder.add(layers.TimeDistributed(layer=layers.Dense(units=features, activation='sigmoid')))\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Deep Dense Autoencoder Model\n",
    "class DAE_Dense(Model):\n",
    "    def __init__(self, input_size, dense_units):\n",
    "        super(DAE_Dense, self).__init__()\n",
    "        self.encoder = get_Dense_encoder(input_size, dense_units)\n",
    "        self.decoder = get_Dense_decoder(input_size, dense_units)\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        encoded = self.encoder(x, training=training)\n",
    "        decoded = self.decoder(encoded, training=training)\n",
    "        return decoded\n",
    "\n",
    "    def encode(self, x, training=False):\n",
    "        encoded = self.encoder(x, training=training)\n",
    "        return encoded\n",
    "    \n",
    "    def decode(self, x, training=False):\n",
    "        decoded = self.decoder(x, training=training)\n",
    "        return decoded\n",
    "    \n",
    "    def train_step(self, x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Reconstruct input\n",
    "            x_encoded = self.encode(x, training=True)\n",
    "            x_recon = self.decode(x_encoded, training=True)\n",
    "            # Calculate loss\n",
    "            loss = self.compiled_loss(x, x_recon)\n",
    "            \n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(x, x_recon)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "### Deep LSTM Autoencoder Model\n",
    "class DAE_LSTM(Model):\n",
    "    def __init__(self, timesteps, features, LSTM_units, LSTM_dropout):\n",
    "        super(DAE_LSTM, self).__init__()\n",
    "        self.encoder = get_LSTM_encoder(timesteps, features, LSTM_units, LSTM_dropout)\n",
    "        self.decoder = get_LSTM_decoder(timesteps, features, LSTM_units, LSTM_dropout)\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        encoded = self.encoder(x, training=training)\n",
    "        decoded = self.decoder(encoded, training=training)\n",
    "        return decoded\n",
    "\n",
    "    def encode(self, x, training=False):\n",
    "        encoded = self.encoder(x, training=training)\n",
    "        return encoded\n",
    "    \n",
    "    def decode(self, x, training=False):\n",
    "        decoded = self.decoder(x, training=training)\n",
    "        return decoded\n",
    "    \n",
    "    def train_step(self, x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Reconstruct input\n",
    "            x_encoded = self.encode(x, training=True)\n",
    "            x_recon = self.decode(x_encoded, training=True)\n",
    "            # Calculate loss\n",
    "            loss = self.compiled_loss(x, x_recon)\n",
    "            \n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(x, x_recon)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    \n",
    "### Deep GRU Autoencoder Model\n",
    "class DAE_GRU(Model):\n",
    "    def __init__(self, timesteps, features, GRU_units, GRU_dropout):\n",
    "        super(DAE_GRU, self).__init__()\n",
    "        self.encoder = get_GRU_encoder(timesteps, features, GRU_units, GRU_dropout)\n",
    "        self.decoder = get_GRU_decoder(timesteps, features, GRU_units, GRU_dropout)\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        encoded = self.encoder(x, training=training)\n",
    "        decoded = self.decoder(encoded, training=training)\n",
    "        return decoded\n",
    "\n",
    "    def encode(self, x, training=False):\n",
    "        encoded = self.encoder(x, training=training)\n",
    "        return encoded\n",
    "    \n",
    "    def decode(self, x, training=False):\n",
    "        decoded = self.decoder(x, training=training)\n",
    "        return decoded\n",
    "    \n",
    "    def train_step(self, x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Reconstruct input\n",
    "            x_encoded = self.encode(x, training=True)\n",
    "            x_recon = self.decode(x_encoded, training=True)\n",
    "            # Calculate loss\n",
    "            loss = self.compiled_loss(x, x_recon)\n",
    "            \n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(x, x_recon)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Robust Autoencoder Model\n",
    "class RobustAutoencoder:\n",
    "    def __init__(self, AE_type: str, prox_type: str, input_size=784, dense_units=[200, 10], lr=3e-4, timesteps=24, features=1, LSTM_units=[64, 32], LSTM_dropout=0.0, GRU_units=[64,32], GRU_dropout=0.0):\n",
    "        super(RobustAutoencoder, self).__init__()\n",
    "        assert AE_type=='Dense' or AE_type=='LSTM' or AE_type=='GRU', 'AE_type has to be either Dense or LSTM or GRU'\n",
    "        self.AE_type = AE_type\n",
    "        \n",
    "        assert prox_type=='l1' or prox_type=='l21', 'prox_type has to be either l1 or l21'\n",
    "        self.prox_type = prox_type\n",
    "        \n",
    "        if self.AE_type=='Dense':\n",
    "            self.AE = DAE_Dense(input_size, dense_units)\n",
    "            self.AE.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                loss='mse',\n",
    "                metrics=['mse']\n",
    "            )\n",
    "            \n",
    "        elif self.AE_type=='LSTM':\n",
    "            self.AE = DAE_LSTM(timesteps, features, LSTM_units, LSTM_dropout)\n",
    "            self.AE.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                loss='mse',\n",
    "                metrics=['mse']\n",
    "            )\n",
    "            \n",
    "        elif self.AE_type=='GRU':\n",
    "            self.AE = DAE_GRU(timesteps, features, GRU_units, GRU_dropout)\n",
    "            self.AE.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                loss='mse',\n",
    "                metrics=['mse']\n",
    "            )\n",
    "        \n",
    "        if self.prox_type=='l1':\n",
    "            self.prox_fn = prox_l1\n",
    "        elif self.prox_type=='l21':\n",
    "            self.prox_fn = prox_l21\n",
    "            \n",
    "    def train_and_fit(self, X, train_iter: int, AE_train_iter: int, batch_size: int, eps: float, lam: float, verbose=0):\n",
    "        if self.AE_type == 'Dense':\n",
    "            self.default_shape = (X.shape[0], X.shape[1])\n",
    "            self.utils_shape = (X.shape[0], X.shape[1])\n",
    "        elif self.AE_type == 'LSTM' or self.AE_type == 'GRU':\n",
    "            self.default_shape = (X.shape[0], X.shape[1], 1)\n",
    "            self.utils_shape = (X.shape[0], X.shape[1])\n",
    "        \n",
    "        X = X.reshape(self.default_shape)\n",
    "        self.L = np.zeros(self.default_shape)\n",
    "        self.S = np.zeros(self.default_shape)\n",
    "        self.LD = np.zeros(self.default_shape)\n",
    "        self.LS = X\n",
    "        \n",
    "        for i in range(train_iter):\n",
    "            if verbose!= 0:\n",
    "                print(f'RAE training iteration: {i+1}')\n",
    "            self.LD = X - self.S\n",
    "            # Now fit the autoencoder for some iters\n",
    "            self.AE.fit(x=self.LD, batch_size=batch_size, epochs=AE_train_iter, verbose=verbose)\n",
    "            self.LD = self.AE(self.LD).numpy()\n",
    "            self.S = X - self.LD\n",
    "            \n",
    "            self.S = self.S.reshape(self.utils_shape)\n",
    "            self.S = self.prox_fn(lam=lam, x=self.S.T).T\n",
    "            self.S = self.S.reshape(self.default_shape)\n",
    "            \n",
    "            c1 = tf.linalg.norm(X - self.LD - self.S) / tf.linalg.norm(X)\n",
    "            c2 = tf.linalg.norm(self.LS - self.LD - self.S) / tf.linalg.norm(X)\n",
    "            if c1 < eps or c2 < eps:\n",
    "                print(f'Early Convergence at iter {i+1}')\n",
    "                break\n",
    "            self.LS = self.LD + self.S\n",
    "        return self.LD, self.S\n",
    "    \n",
    "    def get_reconstruction(self, X):\n",
    "        return self.AE(X)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        L = X - self.S\n",
    "        return self.AE.encode(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L21 Experiment on Time series\n",
    "## Dense RDAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22464,)\n",
      "(22321, 144)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join('data', 'realKnownCause', 'machine_temperature_system_failure.csv'), delimiter=',', decimal='.')\n",
    "df.drop_duplicates(subset='timestamp', keep='first', inplace=True)\n",
    "ts_timestamps = df.iloc[:,0].values[33:-186]\n",
    "ts_values = np.array(df.iloc[:,1].values[33:-186])\n",
    "print(ts_values.shape)\n",
    "#ts_values_daily = ts_values.copy().reshape((int(ts_values.shape[0]/(12*24)), 12*24, 1))\n",
    "\n",
    "timesteps = 144\n",
    "ts_data = subsequences(ts_values, timesteps)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "ts_data_scaled = scaler.fit_transform(X=ts_data)\n",
    "\n",
    "ts_train_dense = ts_data_scaled.copy()\n",
    "np.random.shuffle(ts_train_dense)\n",
    "ts_train_LSTM = ts_data_scaled.reshape((ts_data_scaled.shape[0], timesteps, 1))\n",
    "np.random.shuffle(ts_train_LSTM)\n",
    "print(ts_train_dense.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAEl21Dense_ts = RobustAutoencoder(AE_type='Dense', prox_type='l21', input_size=ts_train_dense.shape[1], dense_units=[60, 20], lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAE training iteration: 1\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 2s 3ms/step - loss: 0.0146 - mse: 0.0146\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 0.0038 - mse: 0.0038\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 0.0017 - mse: 0.0017\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 0.0015 - mse: 0.0015\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 0.0012 - mse: 0.0012\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 9.2043e-04 - mse: 9.2043e-04\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 9.2317e-04 - mse: 9.2317e-04\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 8.5006e-04 - mse: 8.5006e-04\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 8.0947e-04 - mse: 8.0947e-04\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 7.6995e-04 - mse: 7.6995e-04\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 7.5458e-04 - mse: 7.5458e-04\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 6.8057e-04 - mse: 6.8057e-04\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 5.4992e-04 - mse: 5.4992e-04\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 5.5999e-04 - mse: 5.5999e-04\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 5.1379e-04 - mse: 5.1379e-04\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 5.4204e-04 - mse: 5.4204e-04\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 5.0789e-04 - mse: 5.0789e-04\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 5.0093e-04 - mse: 5.0093e-04\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4.8317e-04 - mse: 4.8317e-04\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4.9600e-04 - mse: 4.9600e-04\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4.6434e-04 - mse: 4.6434e-04\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 4.5669e-04 - mse: 4.5669e-04\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 4.5206e-04 - mse: 4.5206e-04\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4.5823e-04 - mse: 4.5823e-04\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4.0467e-04 - mse: 4.0467e-04\n",
      "RAE training iteration: 2\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4.5024e-04 - mse: 4.5024e-04\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4.2329e-04 - mse: 4.2329e-04\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.9411e-04 - mse: 3.9411e-04\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4.1444e-04 - mse: 4.1444e-04\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.7184e-04 - mse: 3.7184e-04\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.9247e-04 - mse: 3.9247e-04\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.9323e-04 - mse: 3.9323e-04\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.7364e-04 - mse: 3.7364e-04\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.4173e-04 - mse: 3.4173e-04\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 4.3512e-04 - mse: 4.3512e-04\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.5482e-04 - mse: 3.5482e-04\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.3366e-04 - mse: 3.3366e-04\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.5585e-04 - mse: 3.5585e-04\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.5453e-04 - mse: 3.5453e-04\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.5148e-04 - mse: 3.5148e-04\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.6195e-04 - mse: 3.6195e-04\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.5956e-04 - mse: 3.5956e-04\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.0352e-04 - mse: 3.0352e-04\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.2137e-04 - mse: 3.2137e-04\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.8594e-04 - mse: 3.8594e-04\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.9512e-04 - mse: 2.9512e-04\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.2143e-04 - mse: 3.2143e-04\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.2308e-04 - mse: 3.2308e-04\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.9696e-04 - mse: 2.9696e-04\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 3.2608e-04 - mse: 3.2608e-04\n",
      "RAE training iteration: 3\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 3.0494e-04 - mse: 3.0494e-04\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 3.0975e-04 - mse: 3.0975e-04\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.9653e-04 - mse: 2.9653e-04\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 3.0982e-04 - mse: 3.0982e-04\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 3.1661e-04 - mse: 3.1661e-04\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.9410e-04 - mse: 2.9410e-04\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.9672e-04 - mse: 2.9672e-04\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.9481e-04 - mse: 2.9481e-04\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.8280e-04 - mse: 2.8280e-04\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.9348e-04 - mse: 2.9348e-04\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.9657e-04 - mse: 2.9657e-04\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.8971e-04 - mse: 2.8971e-04\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.8506e-04 - mse: 2.8506e-04\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.8559e-04 - mse: 2.8559e-04\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.8435e-04 - mse: 2.8435e-04\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.2582e-04 - mse: 3.2582e-04\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.6895e-04 - mse: 2.6895e-04\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.7777e-04 - mse: 2.7777e-04\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.7539e-04 - mse: 2.7539e-04\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.6693e-04 - mse: 2.6693e-04\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.8828e-04 - mse: 2.8828e-04\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.7155e-04 - mse: 2.7155e-04\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.6884e-04 - mse: 2.6884e-04\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.7045e-04 - mse: 2.7045e-04\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.0168e-04 - mse: 3.0168e-04\n",
      "RAE training iteration: 4\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.6637e-04 - mse: 2.6637e-04\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.6166e-04 - mse: 2.6166e-04\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.6791e-04 - mse: 2.6791e-04\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.9231e-04 - mse: 2.9231e-04\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.6267e-04 - mse: 2.6267e-04\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.7505e-04 - mse: 2.7505e-04\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.6160e-04 - mse: 2.6160e-04\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.5495e-04 - mse: 2.5495e-04\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.7780e-04 - mse: 2.7780e-04\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.8425e-04 - mse: 2.8425e-04\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.4954e-04 - mse: 2.4954e-04\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.6064e-04 - mse: 2.6064e-04\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.8406e-04 - mse: 2.8406e-04\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.5036e-04 - mse: 2.5036e-04\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.5425e-04 - mse: 2.5425e-04\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.5697e-04 - mse: 2.5697e-04\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.6057e-04 - mse: 2.6057e-04\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.6539e-04 - mse: 2.6539e-04\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.4767e-04 - mse: 2.4767e-04\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.6318e-04 - mse: 2.6318e-04\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.5067e-04 - mse: 2.5067e-04\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.5288e-04 - mse: 2.5288e-04\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.7128e-04 - mse: 2.7128e-04\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.4785e-04 - mse: 2.4785e-04\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.6355e-04 - mse: 2.6355e-04\n",
      "RAE training iteration: 5\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.8894e-04 - mse: 2.8894e-04\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.3710e-04 - mse: 2.3710e-04\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.5524e-04 - mse: 2.5524e-04\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.5003e-04 - mse: 2.5003e-04\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.4788e-04 - mse: 2.4788e-04\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.5801e-04 - mse: 2.5801e-04\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.4069e-04 - mse: 2.4069e-04\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.3633e-04 - mse: 2.3633e-04\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.8626e-04 - mse: 2.8626e-04\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.3646e-04 - mse: 2.3646e-04\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.4477e-04 - mse: 2.4477e-04\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.3261e-04 - mse: 2.3261e-04\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.4290e-04 - mse: 2.4290e-04\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.3921e-04 - mse: 2.3921e-04\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.5576e-04 - mse: 2.5576e-04\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.4506e-04 - mse: 2.4506e-04\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.2280e-04 - mse: 2.2280e-04\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.4858e-04 - mse: 2.4858e-04\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.2227e-04 - mse: 2.2227e-04\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.1517e-04 - mse: 2.1517e-04\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.4012e-04 - mse: 2.4012e-04\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.2418e-04 - mse: 2.2418e-04\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.1976e-04 - mse: 2.1976e-04\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.1510e-04 - mse: 2.1510e-04\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.3936e-04 - mse: 2.3936e-04\n",
      "RAE training iteration: 6\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.2121e-04 - mse: 2.2121e-04\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.0754e-04 - mse: 2.0754e-04\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.3618e-04 - mse: 2.3618e-04\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.1354e-04 - mse: 2.1354e-04\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.1035e-04 - mse: 2.1035e-04\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0969e-04 - mse: 2.0969e-04\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.4480e-04 - mse: 2.4480e-04\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.1359e-04 - mse: 2.1359e-04\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.3125e-04 - mse: 2.3125e-04\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.0707e-04 - mse: 2.0707e-04\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.1398e-04 - mse: 2.1398e-04\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.0701e-04 - mse: 2.0701e-04\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0896e-04 - mse: 2.0896e-04\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.2105e-04 - mse: 2.2105e-04\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.0971e-04 - mse: 2.0971e-04\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.1303e-04 - mse: 2.1303e-04\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0781e-04 - mse: 2.0781e-04\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0960e-04 - mse: 2.0960e-04\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.0368e-04 - mse: 2.0368e-04\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0611e-04 - mse: 2.0611e-04\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.1490e-04 - mse: 2.1490e-04\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0003e-04 - mse: 2.0003e-04\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.1277e-04 - mse: 2.1277e-04\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.0321e-04 - mse: 2.0321e-04\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0941e-04 - mse: 2.0941e-04\n",
      "RAE training iteration: 7\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.1887e-04 - mse: 2.1887e-04\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0437e-04 - mse: 2.0437e-04\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.1499e-04 - mse: 2.1499e-04\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0812e-04 - mse: 2.0812e-04\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.9492e-04 - mse: 1.9492e-04\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.1254e-04 - mse: 2.1254e-04\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.9717e-04 - mse: 1.9717e-04\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0423e-04 - mse: 2.0423e-04\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0786e-04 - mse: 2.0786e-04\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.9789e-04 - mse: 1.9789e-04\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0198e-04 - mse: 2.0198e-04\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.0149e-04 - mse: 2.0149e-04\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9970e-04 - mse: 1.9970e-04\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9336e-04 - mse: 1.9336e-04\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.1543e-04 - mse: 2.1543e-04\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.0886e-04 - mse: 2.0886e-04\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.9711e-04 - mse: 1.9711e-04\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.0495e-04 - mse: 2.0495e-04\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9452e-04 - mse: 1.9452e-04\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.0176e-04 - mse: 2.0176e-04\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.9938e-04 - mse: 1.9938e-04\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.0304e-04 - mse: 2.0304e-04\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.9304e-04 - mse: 1.9304e-04\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.9849e-04 - mse: 1.9849e-04\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9931e-04 - mse: 1.9931e-04\n",
      "RAE training iteration: 8\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.8996e-04 - mse: 1.8996e-04\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0417e-04 - mse: 2.0417e-04\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9908e-04 - mse: 1.9908e-04\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0041e-04 - mse: 2.0041e-04\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.8951e-04 - mse: 1.8951e-04\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0100e-04 - mse: 2.0100e-04\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.8912e-04 - mse: 1.8912e-04\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0849e-04 - mse: 2.0849e-04\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9873e-04 - mse: 1.9873e-04\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0394e-04 - mse: 2.0394e-04\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.8962e-04 - mse: 1.8962e-04\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.9818e-04 - mse: 1.9818e-04\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9227e-04 - mse: 1.9227e-04\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9907e-04 - mse: 1.9907e-04\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0873e-04 - mse: 2.0873e-04\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.8822e-04 - mse: 1.8822e-04\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.8855e-04 - mse: 1.8855e-04\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.1886e-04 - mse: 2.1886e-04\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.8332e-04 - mse: 1.8332e-04\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9661e-04 - mse: 1.9661e-04\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9232e-04 - mse: 1.9232e-04\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9235e-04 - mse: 1.9235e-04\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9532e-04 - mse: 1.9532e-04\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9421e-04 - mse: 1.9421e-04\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0077e-04 - mse: 2.0077e-04\n",
      "RAE training iteration: 9\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9194e-04 - mse: 1.9194e-04\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0085e-04 - mse: 2.0085e-04\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.8870e-04 - mse: 1.8870e-04\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0115e-04 - mse: 2.0115e-04\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.8796e-04 - mse: 1.8796e-04\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9619e-04 - mse: 1.9619e-04\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.8418e-04 - mse: 1.8418e-04\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0267e-04 - mse: 2.0267e-04\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.9481e-04 - mse: 1.9481e-04\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.8903e-04 - mse: 1.8903e-04\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.9618e-04 - mse: 1.9618e-04\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 6.9775e-04 - mse: 6.9775e-04\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 3.5951e-04 - mse: 3.5951e-04\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.7075e-04 - mse: 2.7075e-04\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.4452e-04 - mse: 2.4452e-04\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.2692e-04 - mse: 2.2692e-04\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.1438e-04 - mse: 2.1438e-04\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0937e-04 - mse: 2.0937e-04\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0845e-04 - mse: 2.0845e-04\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.1016e-04 - mse: 2.1016e-04\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.0389e-04 - mse: 2.0389e-04\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.0563e-04 - mse: 2.0563e-04\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0547e-04 - mse: 2.0547e-04\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9850e-04 - mse: 1.9850e-04\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0778e-04 - mse: 2.0778e-04\n",
      "RAE training iteration: 10\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.0471e-04 - mse: 2.0471e-04\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.1197e-04 - mse: 2.1197e-04\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9740e-04 - mse: 1.9740e-04\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9971e-04 - mse: 1.9971e-04\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0238e-04 - mse: 2.0238e-04\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0464e-04 - mse: 2.0464e-04\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0559e-04 - mse: 2.0559e-04\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0152e-04 - mse: 2.0152e-04\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0155e-04 - mse: 2.0155e-04\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9762e-04 - mse: 1.9762e-04\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9758e-04 - mse: 1.9758e-04\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9718e-04 - mse: 1.9718e-04\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 2.0328e-04 - mse: 2.0328e-04\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0244e-04 - mse: 2.0244e-04\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9388e-04 - mse: 1.9388e-04\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.9500e-04 - mse: 1.9500e-04\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 2.0277e-04 - mse: 2.0277e-04\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9737e-04 - mse: 1.9737e-04\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9529e-04 - mse: 1.9529e-04\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.9994e-04 - mse: 1.9994e-04\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.9563e-04 - mse: 1.9563e-04\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.9552e-04 - mse: 1.9552e-04\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9389e-04 - mse: 1.9389e-04\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.8959e-04 - mse: 1.8959e-04\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.9597e-04 - mse: 1.9597e-04\n"
     ]
    }
   ],
   "source": [
    "LD_l21_dense, S_l21_dense = RAEl21Dense_ts.train_and_fit(X=ts_train_dense, train_iter=10, AE_train_iter=25, batch_size=256, eps=1e-10, lam=lam, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "Detected anomalies: 0\n"
     ]
    }
   ],
   "source": [
    "detected_anomalies_dense = (np.linalg.norm(S_l21_dense, axis=1) > 0.).astype(int)\n",
    "print(detected_anomalies_dense)\n",
    "print(f'Detected anomalies: {np.sum(detected_anomalies_dense)}')\n",
    "\n",
    "non_anomalies = np.argwhere(1-detected_anomalies_dense)\n",
    "np.random.shuffle(non_anomalies)\n",
    "\n",
    "anomalies = np.argwhere(detected_anomalies_dense)\n",
    "np.random.shuffle(anomalies)\n",
    "\n",
    "with open(os.path.join('l21_experiment_ts', 'result'+str(lam)+'.txt'), 'w') as f:\n",
    "    print('Dense stats:', file=f)\n",
    "    print(f'Anomalies: {len(anomalies)}', file=f)\n",
    "    print(f'Non anomalies: {len(non_anomalies)}', file=f)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(non_anomalies)>0:\n",
    "    for i in range(min(len(non_anomalies), 10)):\n",
    "        ind = non_anomalies[i][0]\n",
    "        recon = RAEl21Dense_ts.get_reconstruction(ts_train_dense[ind].reshape((1,-1))).numpy().reshape((-1))\n",
    "        save_ts_recon(ts_train_dense[ind], recon, LD_l21_dense[ind], S_l21_dense[ind], ind+1, zoom=False, filename=os.path.join('l21_experiment_ts', 'lam'+str(lam)+'ts_non_anomaly'+str(ind+1)+'.jpg'))\n",
    "\n",
    "if len(non_anomalies)>0:\n",
    "    for i in range(min(len(non_anomalies), 10)):\n",
    "        ind = non_anomalies[i][0]\n",
    "        recon = RAEl21Dense_ts.get_reconstruction(ts_train_dense[ind].reshape((1,-1))).numpy().reshape((-1))\n",
    "        save_ts_recon(ts_train_dense[ind], recon, LD_l21_dense[ind], S_l21_dense[ind], ind+1, zoom=True, filename=os.path.join('l21_experiment_ts', 'lam'+str(lam)+'ts_non_anomalyzoom'+str(ind+1)+'.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(anomalies)>0:\n",
    "    for i in range(min(len(anomalies), 10)):\n",
    "        ind = anomalies[i][0]\n",
    "        recon = RAEl21Dense_ts.get_reconstruction(ts_train_dense[ind].reshape((1,-1))).numpy().reshape((-1))\n",
    "        save_ts_recon(ts_train_dense[ind], recon, LD_l21_dense[ind], S_l21_dense[ind], ind+1, zoom=False, filename=os.path.join('l21_experiment_ts', 'lam'+str(lam)+'ts_anomaly'+str(ind+1)+'.jpg'))\n",
    "\n",
    "if len(anomalies)>0:\n",
    "    for i in range(min(len(anomalies), 10)):\n",
    "        ind = anomalies[i][0]\n",
    "        recon = RAEl21Dense_ts.get_reconstruction(ts_train_dense[ind].reshape((1,-1))).numpy().reshape((-1))\n",
    "        save_ts_recon(ts_train_dense[ind], recon, LD_l21_dense[ind], S_l21_dense[ind], ind+1, zoom=True, filename=os.path.join('l21_experiment_ts', 'lam'+str(lam)+'ts_anomalyzoom'+str(ind+1)+'.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAEl21LSTM = RobustAutoencoder(AE_type='LSTM', prox_type='l21', timesteps=timesteps, features=8, lr=3e-4, LSTM_dropout=0.0, LSTM_units=[32, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAE training iteration: 1\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 7s 36ms/step - loss: 0.0320 - mse: 0.0320\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.0126 - mse: 0.0126\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.0062 - mse: 0.0062\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.0049 - mse: 0.0049\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.0045 - mse: 0.0045\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.0042 - mse: 0.0042\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.0040 - mse: 0.0040\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.0038 - mse: 0.0038\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0038 - mse: 0.0038\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.0037 - mse: 0.0037\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.0036 - mse: 0.0036\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.0035 - mse: 0.0035\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.0035 - mse: 0.0035\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0034 - mse: 0.0034\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.0034 - mse: 0.0034\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.0034 - mse: 0.0034\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.0034 - mse: 0.0034\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 3s 37ms/step - loss: 0.0033 - mse: 0.0033\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 0.0033 - mse: 0.0033\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.0033 - mse: 0.0033\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.0033 - mse: 0.0033\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.0032 - mse: 0.0032\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.0033 - mse: 0.0033\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.0031 - mse: 0.0031\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.0031 - mse: 0.0031\n",
      "RAE training iteration: 2\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.0495 - mse: 0.0495\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.0491 - mse: 0.0491\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.0490 - mse: 0.0490\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.0488 - mse: 0.0488\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.0489 - mse: 0.0489\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.0487 - mse: 0.0487\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.0486 - mse: 0.0486\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.0484 - mse: 0.0484\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.0483 - mse: 0.0483\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.0482 - mse: 0.0482\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 0.0482 - mse: 0.0482\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.0480 - mse: 0.0480\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.0479 - mse: 0.0479\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.0479 - mse: 0.0479\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 0.0479 - mse: 0.0479\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.0478 - mse: 0.0478\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.0477 - mse: 0.0477\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.0477 - mse: 0.0477\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 0.0476 - mse: 0.0476\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.0477 - mse: 0.0477\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.0475 - mse: 0.0475\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.0475 - mse: 0.0475\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 0.0475 - mse: 0.0475\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.0475 - mse: 0.0475\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.0474 - mse: 0.0474\n",
      "RAE training iteration: 3\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1029 - mse: 0.1029\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1028 - mse: 0.1028\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1028 - mse: 0.1028\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1029 - mse: 0.1029\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1027 - mse: 0.1027\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1028 - mse: 0.1028\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1027 - mse: 0.1027\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1027 - mse: 0.1027\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1027 - mse: 0.1027\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 0.1027 - mse: 0.1027\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1027 - mse: 0.1027\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1027 - mse: 0.1027\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1027 - mse: 0.1027\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1026 - mse: 0.1026\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 0.1028 - mse: 0.1028\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1026 - mse: 0.1026\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1026 - mse: 0.1026\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1026 - mse: 0.1026\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 0.1026 - mse: 0.1026\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1025 - mse: 0.1025\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1025 - mse: 0.1025\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1026 - mse: 0.1026\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 3s 37ms/step - loss: 0.1024 - mse: 0.1024\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1024 - mse: 0.1024\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1024 - mse: 0.1024\n",
      "RAE training iteration: 4\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1031 - mse: 0.1031\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 3s 38ms/step - loss: 0.1032 - mse: 0.1032\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1031 - mse: 0.1031\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1031 - mse: 0.1031\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1031 - mse: 0.1031\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1031 - mse: 0.1031\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1030 - mse: 0.1030\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1031 - mse: 0.1031\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1030 - mse: 0.1030\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1030 - mse: 0.1030\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1030 - mse: 0.1030\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.1030 - mse: 0.1030\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.1030 - mse: 0.1030\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1030 - mse: 0.1030\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 0.1030 - mse: 0.1030\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1030 - mse: 0.1030\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1029 - mse: 0.1029\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1030 - mse: 0.1030\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 0.1030 - mse: 0.1030\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1029 - mse: 0.1029\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1029 - mse: 0.1029\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1029 - mse: 0.1029\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 0.1029 - mse: 0.1029\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1029 - mse: 0.1029\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1029 - mse: 0.1029\n",
      "RAE training iteration: 5\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1034 - mse: 0.1034\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 0.1034 - mse: 0.1034\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1034 - mse: 0.1034\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1034 - mse: 0.1034\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1034 - mse: 0.1034\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1034 - mse: 0.1034\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1034 - mse: 0.1034\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1033 - mse: 0.1033\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1033 - mse: 0.1033\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1033 - mse: 0.1033\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 0.1033 - mse: 0.1033\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1033 - mse: 0.1033\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1033 - mse: 0.1033\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1033 - mse: 0.1033\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 0.1033 - mse: 0.1033\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1033 - mse: 0.1033\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1033 - mse: 0.1033\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1033 - mse: 0.1033\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1033 - mse: 0.1033\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1033 - mse: 0.1033\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1033 - mse: 0.1033\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1033 - mse: 0.1033\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1033 - mse: 0.1033\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1033 - mse: 0.1033\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.1033 - mse: 0.1033\n",
      "RAE training iteration: 6\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1035 - mse: 0.1035\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1035 - mse: 0.1035\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1035 - mse: 0.1035\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1035 - mse: 0.1035\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1035 - mse: 0.1035\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1035 - mse: 0.1035\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 0.1035 - mse: 0.1035\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1035 - mse: 0.1035\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1035 - mse: 0.1035\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.1035 - mse: 0.1035\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 3s 37ms/step - loss: 0.1035 - mse: 0.1035\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 0.1035 - mse: 0.1035\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1035 - mse: 0.1035\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1035 - mse: 0.1035\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 0.1035 - mse: 0.1035\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1035 - mse: 0.1035\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.1035 - mse: 0.1035\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1035 - mse: 0.1035\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1035 - mse: 0.1035\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1035 - mse: 0.1035\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1035 - mse: 0.1035\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1035 - mse: 0.1035\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1035 - mse: 0.1035\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1035 - mse: 0.1035\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.1035 - mse: 0.1035\n",
      "RAE training iteration: 7\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 3s 35ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 3s 37ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 3s 37ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 4s 41ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 3s 36ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 4s 40ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 4s 46ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 4s 49ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 5s 52ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 4s 43ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 5s 55ms/step - loss: 0.1036 - mse: 0.1036\n",
      "RAE training iteration: 8\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 31s 347ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 32s 368ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 30s 340ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 34s 385ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 34s 389ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 33s 377ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 35s 403ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 31s 359ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 28s 319ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 32s 363ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 34s 387ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 8s 83ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 6s 74ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 23s 261ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 27s 310ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 27s 304ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 27s 309ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 27s 303ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 26s 300ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 24s 267ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 25s 287ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 28s 316ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 25s 284ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 27s 308ms/step - loss: 0.1036 - mse: 0.1036\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 25s 282ms/step - loss: 0.1037 - mse: 0.1037\n",
      "RAE training iteration: 9\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 8s 96ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 26s 299ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 28s 313ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 27s 305ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 27s 305ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 27s 303ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 25s 284ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 27s 303ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 25s 283ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 19s 214ms/step - loss: 0.1038 - mse: 0.1038\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 23s 258ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 27s 307ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 26s 292ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 25s 288ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 28s 318ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 22s 251ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 25s 289ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 24s 273ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 26s 300ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 26s 298ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 27s 312ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 27s 306ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 13s 147ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 25s 285ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 26s 297ms/step - loss: 0.1037 - mse: 0.1037\n",
      "RAE training iteration: 10\n",
      "Epoch 1/25\n",
      "88/88 [==============================] - 24s 274ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 2/25\n",
      "88/88 [==============================] - 15s 175ms/step - loss: 0.1038 - mse: 0.1038\n",
      "Epoch 3/25\n",
      "88/88 [==============================] - 27s 304ms/step - loss: 0.1038 - mse: 0.1038\n",
      "Epoch 4/25\n",
      "88/88 [==============================] - 28s 319ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 5/25\n",
      "88/88 [==============================] - 25s 283ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 6/25\n",
      "88/88 [==============================] - 26s 299ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 7/25\n",
      "88/88 [==============================] - 28s 317ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 8/25\n",
      "88/88 [==============================] - 28s 316ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 9/25\n",
      "88/88 [==============================] - 21s 237ms/step - loss: 0.1038 - mse: 0.1038\n",
      "Epoch 10/25\n",
      "88/88 [==============================] - 28s 313ms/step - loss: 0.1038 - mse: 0.1038\n",
      "Epoch 11/25\n",
      "88/88 [==============================] - 26s 300ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 12/25\n",
      "88/88 [==============================] - 28s 320ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 13/25\n",
      "88/88 [==============================] - 26s 290ms/step - loss: 0.1038 - mse: 0.1038\n",
      "Epoch 14/25\n",
      "88/88 [==============================] - 26s 301ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 15/25\n",
      "88/88 [==============================] - 27s 302ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 16/25\n",
      "88/88 [==============================] - 24s 281ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 17/25\n",
      "88/88 [==============================] - 23s 268ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 18/25\n",
      "88/88 [==============================] - 28s 321ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 19/25\n",
      "88/88 [==============================] - 16s 179ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 20/25\n",
      "88/88 [==============================] - 26s 299ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 21/25\n",
      "88/88 [==============================] - 28s 325ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 22/25\n",
      "88/88 [==============================] - 24s 277ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 23/25\n",
      "88/88 [==============================] - 27s 309ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 24/25\n",
      "88/88 [==============================] - 27s 308ms/step - loss: 0.1037 - mse: 0.1037\n",
      "Epoch 25/25\n",
      "88/88 [==============================] - 27s 305ms/step - loss: 0.1037 - mse: 0.1037\n"
     ]
    }
   ],
   "source": [
    "LD_l21_LSTM, S_l21_LSTM = RAEl21LSTM.train_and_fit(X=ts_train_LSTM, AE_train_iter=25, train_iter=10, lam=lam, batch_size=256, eps=1e-10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "Detected anomalies: 9\n"
     ]
    }
   ],
   "source": [
    "from pyexpat import features\n",
    "from time import time\n",
    "\n",
    "\n",
    "shrinked_shape = (ts_train_LSTM.shape[0], ts_train_LSTM.shape[1])\n",
    "\n",
    "detected_anomalies_LSTM = (np.linalg.norm(S_l21_LSTM.reshape(shrinked_shape), axis=1) > 0.).astype(int)\n",
    "print(detected_anomalies_LSTM)\n",
    "print(f'Detected anomalies: {np.sum(detected_anomalies_LSTM)}')\n",
    "\n",
    "shrinked_shape = (ts_train_LSTM.shape[0], ts_train_LSTM.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "Detected anomalies: 9\n"
     ]
    }
   ],
   "source": [
    "detected_anomalies_LSTM = (np.linalg.norm(S_l21_LSTM.reshape(shrinked_shape), axis=1) > 0.).astype(int)\n",
    "print(detected_anomalies_LSTM)\n",
    "print(f'Detected anomalies: {np.sum(detected_anomalies_LSTM)}')\n",
    "\n",
    "non_anomalies = np.argwhere(1-detected_anomalies_LSTM)\n",
    "np.random.shuffle(non_anomalies)\n",
    "\n",
    "anomalies = np.argwhere(detected_anomalies_LSTM)\n",
    "np.random.shuffle(anomalies)\n",
    "\n",
    "with open(os.path.join('l21_experiment_ts', 'result'+str(lam)+'.txt'), 'a') as f:\n",
    "    print('LSTM stats:', file=f)\n",
    "    print(f'Anomalies: {len(anomalies)}', file=f)\n",
    "    print(f'Non anomalies: {len(non_anomalies)}', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "shrinked_shape = (ts_train_LSTM.shape[0], ts_train_LSTM.shape[1])\n",
    "\n",
    "if len(non_anomalies)>0:\n",
    "    for i in range(min(len(non_anomalies), 10)):\n",
    "        ind = non_anomalies[i][0]\n",
    "        recon = RAEl21LSTM.get_reconstruction(ts_train_LSTM[ind].reshape((1,timesteps,1)).reshape((1,timesteps)))\n",
    "        save_ts_recon(ts_train_LSTM[ind].reshape(timesteps),\n",
    "                recon.numpy().reshape((timesteps)),\n",
    "                LD_l21_LSTM[ind],\n",
    "                S_l21_LSTM[ind],\n",
    "                ind+1,\n",
    "                zoom=True,\n",
    "                filename=os.path.join('l21_experiment_ts', 'LSTMlam'+str(lam)+'ts_non_anomaly'+str(ind+1)+'.jpg')\n",
    "        )\n",
    "        \n",
    "if len(non_anomalies)>0:\n",
    "    for i in range(min(len(non_anomalies), 10)):\n",
    "        ind = non_anomalies[i][0]\n",
    "        recon = RAEl21LSTM.get_reconstruction(ts_train_LSTM[ind].reshape((1,timesteps,1)).reshape((1,timesteps)))\n",
    "        save_ts_recon(ts_train_LSTM[ind].reshape(timesteps),\n",
    "                recon.numpy().reshape((timesteps)),\n",
    "                LD_l21_LSTM[ind],\n",
    "                S_l21_LSTM[ind],\n",
    "                ind+1,\n",
    "                zoom=True,\n",
    "                filename=os.path.join('l21_experiment_ts', 'LSTMlam'+str(lam)+'ts_non_anomalyzoom'+str(ind+1)+'.jpg')\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(anomalies)>0:\n",
    "    for i in range(min(len(anomalies), 10)):\n",
    "        ind = anomalies[i][0]\n",
    "        recon = RAEl21LSTM.get_reconstruction(ts_train_LSTM[ind].reshape((1,timesteps,1)).reshape((1,timesteps)))\n",
    "        save_ts_recon(ts_train_LSTM[ind].reshape(timesteps),\n",
    "                recon.numpy().reshape((timesteps)),\n",
    "                LD_l21_LSTM[ind],\n",
    "                S_l21_LSTM[ind],\n",
    "                ind+1,\n",
    "                zoom=False,\n",
    "                filename=os.path.join('l21_experiment_ts', 'LSTMlam'+str(lam)+'ts_anomaly'+str(ind+1)+'.jpg')\n",
    "        )\n",
    "        \n",
    "if len(anomalies)>0:\n",
    "    for i in range(min(len(anomalies), 10)):\n",
    "        ind = anomalies[i][0]\n",
    "        recon = RAEl21LSTM.get_reconstruction(ts_train_LSTM[ind].reshape((1,timesteps,1)).reshape((1,timesteps)))\n",
    "        save_ts_recon(ts_train_LSTM[ind].reshape(timesteps),\n",
    "                recon.numpy().reshape((timesteps)),\n",
    "                LD_l21_LSTM[ind],\n",
    "                S_l21_LSTM[ind],\n",
    "                ind+1,\n",
    "                zoom=True,\n",
    "                filename=os.path.join('l21_experiment_ts', 'LSTMlam'+str(lam)+'ts_anomalyzoom'+str(ind+1)+'.jpg')\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2692c985375d31b3da8770a087597fbdbb725d2d8a937843e14c78467e5dd825"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
